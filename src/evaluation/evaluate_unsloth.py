"""
é›†ä¸­å¼è®­ç»ƒæ¨¡å‹è¯„ä¼° - ä½¿ç”¨GPTè¯„åˆ†
è¯„ä¼°LoRAå¾®è°ƒåçš„æ¨¡å‹åœ¨Javaä»£ç é”™è¯¯æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½

è¯„åˆ†ç»´åº¦ï¼š
1. é”™è¯¯ç§ç±»åŒ¹é…åº¦ (0-10åˆ†)
2. é”™è¯¯ä¸ªæ•°åŒ¹é…åº¦ (0-10åˆ†)  
3. é”™è¯¯å†…å®¹è´¨é‡ (0-10åˆ†)
æ€»åˆ†ï¼š0-30åˆ†
"""

import os
import json
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from typing import Dict, List
from openai import OpenAI
import pandas as pd
from collections import defaultdict
from tqdm import tqdm


class CentralizedModelEvaluator:
    """é›†ä¸­å¼è®­ç»ƒæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        base_model_name: str = "unsloth/Qwen3-8B-Base",
        lora_path: str = "./qwen3_java_evaluator_lora/final_model",
        test_data_path: str = "/mnt/user-data/uploads/test_data.json",
        openai_api_key: str = None,
        gpt_model: str = "gpt-4o-mini",
        output_dir: str = "./centralized_evaluation_results"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            base_model_name: åŸºåº§æ¨¡å‹åç§°
            lora_path: LoRAé€‚é…å™¨è·¯å¾„
            test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
            openai_api_key: OpenAI APIå¯†é’¥
            gpt_model: GPTæ¨¡å‹åç§°
            output_dir: ç»“æœè¾“å‡ºç›®å½•
        """
        self.base_model_name = base_model_name
        self.lora_path = lora_path
        self.test_data_path = test_data_path
        self.gpt_model = gpt_model
        self.output_dir = output_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if openai_api_key:
            self.client = OpenAI(api_key=openai_api_key)
        else:
            # ä»ç¯å¢ƒå˜é‡è¯»å–
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        print(f"{'='*70}")
        print(f"é›†ä¸­å¼è®­ç»ƒæ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–")
        print(f"{'='*70}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"åŸºåº§æ¨¡å‹: {base_model_name}")
        print(f"LoRAè·¯å¾„: {lora_path}")
        print(f"GPTæ¨¡å‹: {gpt_model}")
        
        # åŠ è½½tokenizer
        print(f"\nåŠ è½½Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            trust_remote_code=True
        )
        print(f"âœ“ TokenizeråŠ è½½å®Œæˆ")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print(f"\nåŠ è½½æµ‹è¯•æ•°æ®...")
        self.test_data = self.load_test_data()
        print(f"âœ“ åŠ è½½ {len(self.test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
    
    def load_test_data(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_data = []
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    test_data.append(json.loads(line))
        return test_data
    
    def load_and_merge_model(self):
        """åŠ è½½å¹¶åˆå¹¶LoRAæ¨¡å‹"""
        print(f"\n{'='*70}")
        print(f"åŠ è½½å¹¶åˆå¹¶æ¨¡å‹")
        print(f"{'='*70}")
        
        if not os.path.exists(self.lora_path):
            raise FileNotFoundError(f"LoRAè·¯å¾„ä¸å­˜åœ¨: {self.lora_path}")
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        print(f"1/3 åŠ è½½åŸºåº§æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ“ åŸºåº§æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 2. åŠ è½½LoRAé€‚é…å™¨
        print(f"\n2/3 åŠ è½½LoRAé€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, self.lora_path)
        print(f"âœ“ LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
        
        # 3. åˆå¹¶æ¨¡å‹
        print(f"\n3/3 åˆå¹¶LoRAä¸åŸºåº§æ¨¡å‹...")
        merged_model = model.merge_and_unload()
        print(f"âœ“ æ¨¡å‹åˆå¹¶å®Œæˆ")
        
        merged_model.eval()
        
        return merged_model
    
    # def generate_response(self, model, system_prompt: str, user_prompt: str) -> str:
    #     """
    #     ç”Ÿæˆæ¨¡å‹å“åº”
        
    #     Args:
    #         model: æ¨¡å‹
    #         system_prompt: ç³»ç»Ÿæç¤ºè¯
    #         user_prompt: ç”¨æˆ·æç¤ºè¯
            
    #     Returns:
    #         æ¨¡å‹ç”Ÿæˆçš„å“åº”
    #     """
    #     messages = [
    #         {"role": "system", "content": system_prompt},
    #         {"role": "user", "content": user_prompt}
    #     ]
        
    #     # æ„å»ºè¾“å…¥
    #     text = self.tokenizer.apply_chat_template(
    #         messages,
    #         tokenize=False,
    #         add_generation_prompt=True
    #     )
        
    #     inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
    #     # ç”Ÿæˆ
    #     with torch.no_grad():
    #         outputs = model.generate(
    #             **inputs,
    #             max_new_tokens=512,
    #             temperature=0.1,
    #             top_p=0.9,
    #             do_sample=True,
    #             pad_token_id=self.tokenizer.eos_token_id
    #         )
        
    #     # åªè§£ç æ–°ç”Ÿæˆçš„tokens
    #     input_length = inputs['input_ids'].shape[1]
    #     generated_tokens = outputs[0][input_length:]
    #     response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        
    #     # æ£€æŸ¥ç©ºè¾“å‡º
    #     if not response:
    #         print(f"\nâš ï¸ è­¦å‘Šï¼šæ¨¡å‹æœªç”Ÿæˆå†…å®¹")
    #         return ""
        
    #     return response

    def generate_response(self, model, system_prompt: str, user_prompt: str) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å“åº”
        
        Args:
            model: æ¨¡å‹
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            æ¨¡å‹ç”Ÿæˆçš„å“åº”
        """
        # æ‰‹åŠ¨æ„å»º Qwen3 chat æ ¼å¼ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        text += f"<|im_start|>user\n{user_prompt}<|im_end|>\n"
        text += "<|im_start|>assistant\n"
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.convert_tokens_to_ids("<|im_end|>")  # æ·»åŠ ç»“æŸç¬¦
            )
        
        # åªè§£ç æ–°ç”Ÿæˆçš„tokens
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        
        # ç§»é™¤å¯èƒ½çš„ç»“æŸæ ‡è®°
        if response.endswith("<|im_end|>"):
            response = response[:-len("<|im_end|>")].strip()
        
        # æ£€æŸ¥ç©ºè¾“å‡º
        if not response:
            print(f"\nâš ï¸ è­¦å‘Šï¼šæ¨¡å‹æœªç”Ÿæˆå†…å®¹")
            return ""
        
        return response
    
    def gpt_score(self, prediction: str, ground_truth: str, retry: int = 3) -> Dict:
        """
        ä½¿ç”¨GPTå¯¹é¢„æµ‹ç»“æœè¿›è¡Œè¯„åˆ†
        
        Args:
            prediction: æ¨¡å‹é¢„æµ‹
            ground_truth: æ ‡å‡†ç­”æ¡ˆ
            retry: é‡è¯•æ¬¡æ•°
            
        Returns:
            è¯„åˆ†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - type_score: é”™è¯¯ç§ç±»å¾—åˆ† (0-10)
            - count_score: é”™è¯¯ä¸ªæ•°å¾—åˆ† (0-10)
            - content_score: é”™è¯¯å†…å®¹å¾—åˆ† (0-10)
            - total_score: æ€»åˆ† (0-30)
            - reasoning: è¯„åˆ†ç†ç”±
        """
        
        # å¤„ç†ç©ºé¢„æµ‹
        if not prediction or prediction.strip() == "":
            return {
                'type_score': 0.0,
                'count_score': 0.0,
                'content_score': 0.0,
                'total_score': 0.0,
                'reasoning': 'æ¨¡å‹æœªç”Ÿæˆä»»ä½•è¾“å‡º'
            }
        
        scoring_prompt = f"""You are an expert code reviewer evaluating error detection results for Java code.

Ground Truth (Standard Answer):
{ground_truth}

Model Prediction:
{prediction}

Please evaluate the prediction based on three dimensions (each worth 10 points):

1. **Error Type Match (10 points)**:
   - Does the prediction correctly identify error types (Syntax Error, Runtime Error, Logical Error)?
   - 10 points: All error types match perfectly
   - 7-9 points: Most error types match with minor discrepancies
   - 4-6 points: Some error types match but with significant issues
   - 1-3 points: Few error types match
   - 0 points: Completely wrong or opposite

2. **Error Count Match (10 points)**:
   - Does the prediction identify the correct number of errors?
   - 10 points: Exact same number of errors
   - 7-9 points: Off by 1 error
   - 4-6 points: Off by 2 errors
   - 1-3 points: Off by 3+ errors
   - 0 points: Completely wrong (e.g., says "no errors" when there are errors, or vice versa)

3. **Error Content Quality (10 points)**:
   - How accurate and complete are the error descriptions?
   - 10 points: Descriptions are accurate, specific, and match the ground truth closely
   - 7-9 points: Descriptions are mostly accurate with minor missing details
   - 4-6 points: Descriptions capture the main idea but lack precision
   - 1-3 points: Descriptions are vague or partially incorrect
   - 0 points: Descriptions are wrong or missing

**Important Notes**:
- If ground truth says "No errors found. Code is correct." or similar, and prediction also identifies no errors, give full marks.
- Focus on semantic meaning, not exact wording.
- Minor phrasing differences should not heavily penalize scores.

Return your evaluation in the following JSON format:
{{
    "type_score": <float 0-10>,
    "count_score": <float 0-10>,
    "content_score": <float 0-10>,
    "reasoning": "<brief explanation of your scoring>"
}}

Only return the JSON, no additional text."""

        for attempt in range(retry):
            try:
                response = self.client.chat.completions.create(
                    model=self.gpt_model,
                    messages=[{"role": "user", "content": scoring_prompt}],
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                result_text = response.choices[0].message.content
                result = json.loads(result_text)
                
                # è®¡ç®—æ€»åˆ†
                result['total_score'] = (
                    result['type_score'] + 
                    result['count_score'] + 
                    result['content_score']
                )
                
                return result
                
            except Exception as e:
                print(f"\nâš ï¸ GPTè¯„åˆ†å¤±è´¥ (å°è¯• {attempt + 1}/{retry}): {e}")
                if attempt < retry - 1:
                    time.sleep(2)
                else:
                    # è¿”å›é»˜è®¤åˆ†æ•°
                    return {
                        'type_score': 0.0,
                        'count_score': 0.0,
                        'content_score': 0.0,
                        'total_score': 0.0,
                        'reasoning': f"è¯„åˆ†å¤±è´¥: {str(e)}"
                    }
    
    def evaluate_model(self, model) -> List[Dict]:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            model: å·²åŠ è½½çš„æ¨¡å‹
            
        Returns:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        print(f"\n{'='*70}")
        print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹")
        print(f"{'='*70}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(self.test_data)}")
        
        results = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for idx, sample in enumerate(tqdm(self.test_data, desc="è¯„ä¼°è¿›åº¦")):

            # ç”Ÿæˆé¢„æµ‹
            prediction = self.generate_response(
                model,
                sample['system_prompt'],
                sample['user_prompt']
            )
            
            # GPTè¯„åˆ†
            scores = self.gpt_score(prediction, sample['feedback'])
            
            # ä¿å­˜ç»“æœ
            result = {
                'index': idx,
                'system_prompt': sample['system_prompt'],
                'user_prompt': sample['user_prompt'],
                'ground_truth': sample['feedback'],
                'prediction': prediction,
                'scores': scores
            }
            
            results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…APIé™æµ
            if (idx + 1) % 10 == 0:
                time.sleep(1)
        
        print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼")
        
        return results
    
    def calculate_statistics(self, results: List[Dict]) -> Dict:
        """
        è®¡ç®—ç»Ÿè®¡æ•°æ®
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        stats = {
            'total_samples': len(results),
            'avg_type_score': 0.0,
            'avg_count_score': 0.0,
            'avg_content_score': 0.0,
            'avg_total_score': 0.0,
            'std_total_score': 0.0,
            'score_distribution': {
                'type': defaultdict(int),
                'count': defaultdict(int),
                'content': defaultdict(int)
            }
        }
        
        type_scores = []
        count_scores = []
        content_scores = []
        total_scores = []
        
        for result in results:
            scores = result['scores']
            type_scores.append(scores['type_score'])
            count_scores.append(scores['count_score'])
            content_scores.append(scores['content_score'])
            total_scores.append(scores['total_score'])
            
            # åˆ†æ•°åˆ†å¸ƒï¼ˆæŒ‰2åˆ†ä¸€æ¡£ç»Ÿè®¡ï¼‰
            stats['score_distribution']['type'][int(scores['type_score'] // 2) * 2] += 1
            stats['score_distribution']['count'][int(scores['count_score'] // 2) * 2] += 1
            stats['score_distribution']['content'][int(scores['content_score'] // 2) * 2] += 1
        
        stats['avg_type_score'] = sum(type_scores) / len(type_scores)
        stats['avg_count_score'] = sum(count_scores) / len(count_scores)
        stats['avg_content_score'] = sum(content_scores) / len(content_scores)
        stats['avg_total_score'] = sum(total_scores) / len(total_scores)
        
        # è®¡ç®—æ ‡å‡†å·®
        mean_total = stats['avg_total_score']
        variance = sum((s - mean_total) ** 2 for s in total_scores) / len(total_scores)
        stats['std_total_score'] = variance ** 0.5
        
        return stats
    
    def print_statistics(self, stats: Dict):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"è¯„ä¼°ç»Ÿè®¡")
        print(f"{'='*70}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"\nå¹³å‡å¾—åˆ†:")
        print(f"  é”™è¯¯ç§ç±»: {stats['avg_type_score']:.2f}/10")
        print(f"  é”™è¯¯ä¸ªæ•°: {stats['avg_count_score']:.2f}/10")
        print(f"  é”™è¯¯å†…å®¹: {stats['avg_content_score']:.2f}/10")
        print(f"  æ€»åˆ†: {stats['avg_total_score']:.2f}/30 ({stats['avg_total_score']/30*100:.1f}%)")
        print(f"  æ ‡å‡†å·®: {stats['std_total_score']:.2f}")
    
    def print_detailed_cases(self, results: List[Dict], num_low: int = 5, num_high: int = 5):
        """
        æ‰“å°è¯¦ç»†æ¡ˆä¾‹
        
        Args:
            results: è¯„ä¼°ç»“æœ
            num_low: ä½åˆ†æ¡ˆä¾‹æ•°é‡
            num_high: é«˜åˆ†æ¡ˆä¾‹æ•°é‡
        """
        # æŒ‰æ€»åˆ†æ’åº
        sorted_results = sorted(results, key=lambda x: x['scores']['total_score'])
        
        # ä½åˆ†æ¡ˆä¾‹
        print(f"\n{'='*70}")
        print(f"ä½åˆ†æ¡ˆä¾‹åˆ†æï¼ˆéœ€è¦æ”¹è¿›ï¼‰")
        print(f"{'='*70}")
        
        for idx, result in enumerate(sorted_results[:num_low]):
            print(f"\n{'-'*70}")
            print(f"æ¡ˆä¾‹ {idx+1} (æ ·æœ¬ #{result['index']}) - æ€»åˆ†: {result['scores']['total_score']:.1f}/30")
            print(f"{'-'*70}")
            
            # ä»»åŠ¡ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
            user_prompt = result['user_prompt']
            if len(user_prompt) > 150:
                user_prompt = user_prompt[:150] + "..."
            print(f"\nã€ä»»åŠ¡ã€‘\n{user_prompt}")
            
            # Ground Truth
            print(f"\nã€æ ‡å‡†ç­”æ¡ˆã€‘\n{result['ground_truth']}")
            
            # æ¨¡å‹é¢„æµ‹
            print(f"\nã€æ¨¡å‹é¢„æµ‹ã€‘\n{result['prediction']}")
            
            # è¯„åˆ†
            scores = result['scores']
            print(f"\nã€GPTè¯„åˆ†ã€‘")
            print(f"  é”™è¯¯ç§ç±»: {scores['type_score']:.1f}/10")
            print(f"  é”™è¯¯ä¸ªæ•°: {scores['count_score']:.1f}/10")
            print(f"  é”™è¯¯å†…å®¹: {scores['content_score']:.1f}/10")
            print(f"  æ€»åˆ†: {scores['total_score']:.1f}/30")
            print(f"  è¯„åˆ†ç†ç”±: {scores['reasoning']}")
        
        # é«˜åˆ†æ¡ˆä¾‹
        print(f"\n{'='*70}")
        print(f"é«˜åˆ†æ¡ˆä¾‹åˆ†æï¼ˆè¡¨ç°ä¼˜ç§€ï¼‰")
        print(f"{'='*70}")
        
        for idx, result in enumerate(sorted_results[-num_high:]):
            print(f"\n{'-'*70}")
            print(f"æ¡ˆä¾‹ {idx+1} (æ ·æœ¬ #{result['index']}) - æ€»åˆ†: {result['scores']['total_score']:.1f}/30")
            print(f"{'-'*70}")
            
            print(f"\nã€æ ‡å‡†ç­”æ¡ˆã€‘\n{result['ground_truth']}")
            print(f"\nã€æ¨¡å‹é¢„æµ‹ã€‘\n{result['prediction']}")
            
            scores = result['scores']
            print(f"\nã€GPTè¯„åˆ†ã€‘æ€»åˆ†: {scores['total_score']:.1f}/30")
            print(f"  (ç§ç±»:{scores['type_score']:.1f} ä¸ªæ•°:{scores['count_score']:.1f} å†…å®¹:{scores['content_score']:.1f})")
    
    def save_results(self, results: List[Dict], stats: Dict):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœ
            stats: ç»Ÿè®¡æ•°æ®
        """
        print(f"\n{'='*70}")
        print(f"ä¿å­˜è¯„ä¼°ç»“æœ")
        print(f"{'='*70}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"{self.output_dir}/detailed_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ“ è¯¦ç»†ç»“æœ: {results_file}")
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = f"{self.output_dir}/statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        print(f"âœ“ ç»Ÿè®¡æ•°æ®: {stats_file}")
        
        # ç”ŸæˆCSVæ‘˜è¦
        summary_data = {
            'Metric': [
                'Error Type Score',
                'Error Count Score',
                'Error Content Score',
                'Total Score',
                'Percentage'
            ],
            'Value': [
                f"{stats['avg_type_score']:.2f}/10",
                f"{stats['avg_count_score']:.2f}/10",
                f"{stats['avg_content_score']:.2f}/10",
                f"{stats['avg_total_score']:.2f}/30",
                f"{stats['avg_total_score']/30*100:.1f}%"
            ]
        }
        
        df = pd.DataFrame(summary_data)
        csv_file = f"{self.output_dir}/summary.csv"
        df.to_csv(csv_file, index=False)
        print(f"âœ“ è¯„ä¼°æ‘˜è¦: {csv_file}")
    
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print(f"\n{'='*70}")
        print(f"å¼€å§‹å®Œæ•´è¯„ä¼°æµç¨‹")
        print(f"{'='*70}")
        
        # 1. åŠ è½½å¹¶åˆå¹¶æ¨¡å‹
        model = self.load_and_merge_model()
        
        # 2. è¯„ä¼°æ¨¡å‹
        results = self.evaluate_model(model)
        
        # 3. è®¡ç®—ç»Ÿè®¡
        stats = self.calculate_statistics(results)
        
        # 4. æ‰“å°ç»Ÿè®¡
        self.print_statistics(stats)
        
        # 5. æ‰“å°è¯¦ç»†æ¡ˆä¾‹
        self.print_detailed_cases(results, num_low=3, num_high=3)
        
        # 6. ä¿å­˜ç»“æœ
        self.save_results(results, stats)
        
        # 7. æ¸…ç†å†…å­˜
        del model
        torch.cuda.empty_cache()
        
        print(f"\n{'='*70}")
        print(f"è¯„ä¼°å®Œæˆï¼")
        print(f"{'='*70}")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}/")
        
        return results, stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é›†ä¸­å¼è®­ç»ƒæ¨¡å‹GPTè¯„åˆ†è¯„ä¼°')
    parser.add_argument('--base_model', type=str, default="unsloth/Qwen3-8B-Base",
                        help='åŸºåº§æ¨¡å‹åç§°')
    parser.add_argument('--lora_path', type=str, 
                        default="./qwen3_java_evaluator_lora_unsloth_low_lr/final_model",
                        
                        help='LoRAé€‚é…å™¨è·¯å¾„')
    parser.add_argument('--test_data', type=str, 
                        default="./data/test_data.json",
                        help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--api_key', type=str, default=None,
                        help='OpenAI API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰')
    parser.add_argument('--gpt_model', type=str, default='gpt-4o-mini',
                        help='GPTæ¨¡å‹åç§°')
    parser.add_argument('--output_dir', type=str, 
                        default='./centralized_evaluation_results_unsloth_low_lr',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CentralizedModelEvaluator(
        base_model_name=args.base_model,
        lora_path=args.lora_path,
        test_data_path=args.test_data,
        openai_api_key=args.api_key,
        gpt_model=args.gpt_model,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œè¯„ä¼°
    results, stats = evaluator.run_evaluation()
    
    print(f"\nğŸ‰ å®Œæˆï¼æŸ¥çœ‹ç»“æœ:")
    print(f"  - è¯¦ç»†ç»“æœ: {args.output_dir}/detailed_results.json")
    print(f"  - ç»Ÿè®¡æ•°æ®: {args.output_dir}/statistics.json")
    print(f"  - è¯„ä¼°æ‘˜è¦: {args.output_dir}/summary.csv")


if __name__ == "__main__":
    main()