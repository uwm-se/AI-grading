import json
import torch
from datasets import Dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments,AutoTokenizer, AutoModelForCausalLM


# ==================== é…ç½®å‚æ•° ====================
# æ¨¡å‹é…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
MODEL_NAME = "unsloth/Qwen3-8B-Base"  # ä½¿ç”¨ Qwen3-8B-Base
MAX_SEQ_LENGTH = 1536  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´

# é‡åŒ–é…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
LOAD_IN_4BIT = True
USE_NESTED_QUANT = True  # ä½¿ç”¨åµŒå¥—é‡åŒ–
USE_GRADIENT_CHECKPOINTING = True

# LoRA å‚æ•°ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
LORA_R = 32 #16  # LoRA rank
LORA_ALPHA = 64 #32  # LoRA alpha
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]

# è®­ç»ƒå‚æ•°ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
OUTPUT_DIR = "./qwen3_java_evaluator_lora_unsloth_low_lr"
NUM_TRAIN_EPOCHS = 5  # å¯¹åº”è”é‚¦å­¦ä¹ çš„ LOCAL_EPOCHS * NUM_ROUNDS = 3 * 1
PER_DEVICE_TRAIN_BATCH_SIZE = 1  # ä¸è”é‚¦å­¦ä¹ å•ä¸ªclientä¸€è‡´
PER_DEVICE_EVAL_BATCH_SIZE = 1  # ä¸è®­ç»ƒbatch sizeä¿æŒä¸€è‡´
GRADIENT_ACCUMULATION_STEPS = 8  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼ˆæœ‰æ•ˆ batch size = 1 * 8 = 8ï¼‰
LEARNING_RATE = 5e-5  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´
WARMUP_STEPS = 100
LOGGING_STEPS = 10
EVAL_STEPS = 50
SAVE_STEPS = 100

# ==================== åŠ è½½æ•°æ® ====================
def load_json_data(file_path):
    """åŠ è½½ JSONL æ ¼å¼æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def format_chat_template(sample):
    """å°†æ•°æ®æ ¼å¼åŒ–ä¸º Qwen çš„ chat æ ¼å¼"""
    messages = [
        {"role": "system", "content": sample["system_prompt"]},
        {"role": "user", "content": sample["user_prompt"]},
        {"role": "assistant", "content": sample["feedback"]}
    ]
    return {"messages": messages}

def formatting_prompts_func(examples):
    texts = []
    for messages in examples["messages"]:
        text = ""
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                text += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                text += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                text += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        texts.append(text)
    return {"text": texts}

print("ğŸ“š åŠ è½½æ•°æ®...")
train_data = load_json_data("./data/new_train_data.json")
valid_data = load_json_data("./data/valid_data.json")

print(f"è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
print(f"éªŒè¯æ•°æ®: {len(valid_data)} æ¡")

# è½¬æ¢ä¸º Dataset
train_dataset = Dataset.from_list([format_chat_template(d) for d in train_data])
valid_dataset = Dataset.from_list([format_chat_template(d) for d in valid_data])
train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
valid_dataset = valid_dataset.map(formatting_prompts_func, batched=True)

print(train_dataset[:2])

# ==================== åŠ è½½æ¨¡å‹ ====================
print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {MODEL_NAME}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # è‡ªåŠ¨é€‰æ‹©
    load_in_4bit=LOAD_IN_4BIT,
)


# é…ç½® LoRA
print("ğŸ”§ é…ç½® LoRA...")
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=LORA_TARGET_MODULES,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    use_gradient_checkpointing="unsloth" if USE_GRADIENT_CHECKPOINTING else False,
    random_state=42,
)

# æ‰“å°å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)")

# ==================== è®­ç»ƒé…ç½® ====================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    learning_rate=LEARNING_RATE,
    warmup_steps=WARMUP_STEPS,
    logging_steps=LOGGING_STEPS,
    eval_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_strategy="steps",
    save_steps=SAVE_STEPS,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    optim="adamw_8bit",  # 8-bit AdamW èŠ‚çœæ˜¾å­˜
    weight_decay=0.01,
    lr_scheduler_type="cosine",
    gradient_checkpointing=True,
    report_to="none",  # ä¸ä½¿ç”¨ wandb
)

# ==================== åˆ›å»º Trainer ====================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    args=training_args,
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_text_field="text",
    #dataset_text_field="messages",  # ä½¿ç”¨ messages å­—æ®µ
    packing=False,  # ä¸ä½¿ç”¨ packingï¼Œä¿æŒå¯¹è¯å®Œæ•´æ€§
)

# ==================== å¼€å§‹è®­ç»ƒ ====================
print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()

# ==================== ä¿å­˜æ¨¡å‹ ====================
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained(f"{OUTPUT_DIR}/final_model")
tokenizer.save_pretrained(f"{OUTPUT_DIR}/final_model")


print("\nâœ… è®­ç»ƒå®Œæˆï¼")
print(f"æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}/final_model")
