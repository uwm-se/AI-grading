import os
import torch
import json
import time
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    Trainer,
    default_data_collator,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from datasets import load_dataset
import numpy as np


# ==================== é…ç½® ====================
class CentralizedConfig:
    """é›†ä¸­å¼è®­ç»ƒé…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ è¶…å‚æ•°å®Œå…¨ä¸€è‡´ï¼‰"""
    
    # æ¨¡å‹é…ç½®
    MODEL_NAME = "Qwen/Qwen3-8B-Base"
    
    # LoRAé…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.05
    LORA_TARGET_MODULES = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
    
    # è®­ç»ƒé…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
    NUM_EPOCHS = 3  # å¯¹åº”è”é‚¦å­¦ä¹ çš„ LOCAL_EPOCHS * NUM_ROUNDS = 3 * 1
    BATCH_SIZE = 1  # ä¸è”é‚¦å­¦ä¹ å•ä¸ªclientä¸€è‡´
    GRADIENT_ACCUMULATION_STEPS = 8  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´
    LEARNING_RATE = 1e-4  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´
    MAX_SEQ_LENGTH = 1536  # ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´
    
    # é‡åŒ–é…ç½®ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
    USE_4BIT = True
    USE_NESTED_QUANT = True
    USE_GRADIENT_CHECKPOINTING = True
    
    # æ•°æ®é…ç½®
    DATA_DIR = "./data"
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = "./java_error_centralized_results_8b"


def check_gpu_capability():
    """æ£€æŸ¥GPUèƒ½åŠ›"""
    if not torch.cuda.is_available():
        raise RuntimeError("éœ€è¦GPUæ‰èƒ½è¿è¡Œæ­¤è„šæœ¬")
    
    device_name = torch.cuda.get_device_name(0)
    compute_capability = torch.cuda.get_device_capability(0)
    memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"\næ£€æµ‹åˆ°GPU:")
    print(f"  è®¾å¤‡: {device_name}")
    print(f"  è®¡ç®—èƒ½åŠ›: {compute_capability[0]}.{compute_capability[1]}")
    print(f"  æ˜¾å­˜: {memory:.1f} GB")
    
    # åˆ¤æ–­æ˜¯å¦æ”¯æŒbfloat16
    supports_bf16 = compute_capability[0] >= 8
    
    if supports_bf16:
        print(f"  âœ… GPUæ”¯æŒbfloat16")
        compute_dtype = torch.bfloat16
        use_bf16 = True
        use_fp16 = False
    else:
        print(f"  âš ï¸ GPUä¸æ”¯æŒbfloat16ï¼Œä½¿ç”¨float16")
        compute_dtype = torch.float16
        use_bf16 = False
        use_fp16 = True
    
    return compute_dtype, use_bf16, use_fp16


# ==================== æ•°æ®åŠ è½½ ====================
def preprocess_function(examples, tokenizer, max_length=1536):
    """é¢„å¤„ç†å‡½æ•°ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰"""
    texts = []
    for messages in examples["messages"]:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(text)
    
    model_inputs = tokenizer(
        texts,
        max_length=max_length,
        truncation=True,
        padding="max_length",
        return_tensors=None,
    )
    
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs


def load_train_data(data_dir: str, tokenizer, max_length: int):
    """
    åŠ è½½è®­ç»ƒæ•°æ®
    ç›´æ¥ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒé›†ï¼ˆè”é‚¦å­¦ä¹ ä¸­è¢«åˆ†å‰²ä¸ºå¤šä¸ªclientï¼‰
    """
    print(f"\nåŠ è½½è®­ç»ƒæ•°æ®...")
    
    train_file = f"{data_dir}/new_train_data_message.json"
    
    if not os.path.exists(train_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {train_file}")
    
    dataset = load_dataset('json', data_files=train_file, split='train')
    
    print(f"  åŸå§‹æ ·æœ¬æ•°: {len(dataset)}")
    
    # é¢„å¤„ç†
    dataset = dataset.map(
        lambda examples: preprocess_function(examples, tokenizer, max_length),
        batched=True,
        remove_columns=dataset.column_names,
        desc="Tokenizing training data"
    )
    
    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    
    print(f"\nâœ… è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"  æ³¨: è”é‚¦å­¦ä¹ ä¸­æ­¤æ•°æ®è¢«åˆ†å‰²ä¸ºå¤šä¸ªå®¢æˆ·ç«¯")
    
    return dataset


def load_validation_data(data_dir: str, tokenizer, max_length: int):
    """
    åŠ è½½éªŒè¯é›†æ•°æ®
    """
    val_file = f"{data_dir}/valid.json"
    
    if not os.path.exists(val_file):
        print(f"\nâš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°éªŒè¯é›†æ–‡ä»¶ {val_file}")
        return None
    
    print(f"\nåŠ è½½éªŒè¯é›†...")
    
    dataset = load_dataset('json', data_files=val_file, split='train')
    
    # é¢„å¤„ç†
    dataset = dataset.map(
        lambda examples: preprocess_function(examples, tokenizer, max_length),
        batched=True,
        remove_columns=dataset.column_names,
        desc="Tokenizing validation set"
    )
    
    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    
    print(f"âœ… éªŒè¯é›†åŠ è½½å®Œæˆ:")
    print(f"  æ ·æœ¬æ•°: {len(dataset)}")
    
    return dataset


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_centralized(
    model,
    tokenizer,
    train_dataset,
    eval_dataset,
    config: CentralizedConfig,
    precision_info: dict
):
    """
    é›†ä¸­å¼è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†è¯„ä¼°ï¼‰
    è¶…å‚æ•°ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´
    """
    
    print(f"\n{'='*70}")
    print(f"å¼€å§‹é›†ä¸­å¼è®­ç»ƒï¼ˆCentralized Training with Validationï¼‰")
    print(f"{'='*70}")
    
    # è®¾ç½®è¯„ä¼°ç­–ç•¥
    if eval_dataset is not None:
        evaluation_strategy = "steps"  # æ¯Næ­¥è¯„ä¼°ä¸€æ¬¡
        eval_steps = 50  # å¯ä»¥æ ¹æ®æ•°æ®é‡è°ƒæ•´
        load_best_model_at_end = True
        metric_for_best_model = "eval_loss"
        greater_is_better = False
        print(f"âœ… å¯ç”¨éªŒè¯é›†è¯„ä¼° (æ¯ {eval_steps} æ­¥)")
    else:
        evaluation_strategy = "no"
        eval_steps = None
        load_best_model_at_end = False
        metric_for_best_model = None
        greater_is_better = None
        print(f"âš ï¸  æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œä¸è¿›è¡Œè¯„ä¼°")
    
    # è®­ç»ƒå‚æ•°ï¼ˆä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼‰
    training_args = TrainingArguments(
        output_dir=f"{config.OUTPUT_DIR}/checkpoints",
        num_train_epochs=config.NUM_EPOCHS,
        per_device_train_batch_size=config.BATCH_SIZE,
        per_device_eval_batch_size=config.BATCH_SIZE,  # éªŒè¯æ—¶ä¹Ÿç”¨ç›¸åŒbatch size
        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
        learning_rate=config.LEARNING_RATE,
        fp16=precision_info['use_fp16'],
        bf16=precision_info['use_bf16'],
        optim="paged_adamw_8bit",
        gradient_checkpointing=config.USE_GRADIENT_CHECKPOINTING,
        logging_steps=10,
        logging_dir=f"{config.OUTPUT_DIR}/logs",
        save_strategy="steps" if eval_dataset is not None else "epoch",
        save_steps=eval_steps if eval_dataset is not None else None,
        save_total_limit=3,
        eval_strategy=evaluation_strategy,
        eval_steps=eval_steps,
        load_best_model_at_end=load_best_model_at_end,
        metric_for_best_model=metric_for_best_model,
        greater_is_better=greater_is_better,
        report_to="none",
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        dataloader_pin_memory=True,
        dataloader_num_workers=4,
        remove_unused_columns=False,
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = default_data_collator
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # æ·»åŠ éªŒè¯é›†
        data_collator=data_collator,
    )
    
    # è®­ç»ƒå‰ä¿¡æ¯
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset is not None:
        print(f"  éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")
    print(f"  è®­ç»ƒè½®æ¬¡: {config.NUM_EPOCHS}")
    print(f"  Batch Size: {config.BATCH_SIZE}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.GRADIENT_ACCUMULATION_STEPS}")
    print(f"  å®é™…Batch Size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
    print(f"  å­¦ä¹ ç‡: {config.LEARNING_RATE}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {config.MAX_SEQ_LENGTH}")
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    result = trainer.train()
    end_time = time.time()
    
    training_time = end_time - start_time
    
    # æœ€ç»ˆè¯„ä¼°
    final_metrics = {}
    if eval_dataset is not None:
        print(f"\nè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        eval_results = trainer.evaluate()
        final_metrics = eval_results
        print(f"âœ… æœ€ç»ˆéªŒè¯Loss: {eval_results['eval_loss']:.4f}")
    
    # è®­ç»ƒå†å²
    training_history = {
        "method": "centralized",
        "model": config.MODEL_NAME,
        "total_train_samples": len(train_dataset),
        "total_eval_samples": len(eval_dataset) if eval_dataset is not None else 0,
        "num_epochs": config.NUM_EPOCHS,
        "batch_size": config.BATCH_SIZE,
        "gradient_accumulation_steps": config.GRADIENT_ACCUMULATION_STEPS,
        "learning_rate": config.LEARNING_RATE,
        "max_seq_length": config.MAX_SEQ_LENGTH,
        "final_train_loss": result.training_loss,
        "final_eval_metrics": final_metrics,
        "total_training_time": training_time,
        "log_history": trainer.state.log_history
    }
    
    print(f"\n{'='*70}")
    print(f"è®­ç»ƒå®Œæˆ!")
    print(f"{'='*70}")
    print(f"  æœ€ç»ˆè®­ç»ƒLoss: {result.training_loss:.4f}")
    if eval_dataset is not None:
        print(f"  æœ€ç»ˆéªŒè¯Loss: {final_metrics.get('eval_loss', 'N/A'):.4f}")
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’ ({training_time/60:.1f}åˆ†é’Ÿ)")
    print(f"  å¹³å‡æ¯epoch: {training_time/config.NUM_EPOCHS:.1f}ç§’")
    
    return model, training_history


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    
    print("="*70)
    print("é›†ä¸­å¼è®­ç»ƒ (Centralized Training)")
    print("ä½œä¸ºè”é‚¦å­¦ä¹ çš„Baselineå¯¹æ¯”")
    print("="*70)
    print("âš ï¸  æ³¨æ„: æ‰€æœ‰è¶…å‚æ•°ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´")
    print("="*70)
    
    config = CentralizedConfig()
    
    # æ£€æŸ¥GPU
    compute_dtype, use_bf16, use_fp16 = check_gpu_capability()
    
    precision_info = {
        'use_bf16': use_bf16,
        'use_fp16': use_fp16,
        'compute_dtype': str(compute_dtype)
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–tokenizer
    print("\nåˆå§‹åŒ–Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        config.MODEL_NAME,
        trust_remote_code=True,
        padding_side="right",
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ“ Tokenizeråˆå§‹åŒ–å®Œæˆ")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_dataset = load_train_data(
        config.DATA_DIR,
        #"./data/new_train_data_message.json",
        tokenizer,
        config.MAX_SEQ_LENGTH
    )
    
    # åŠ è½½éªŒè¯é›†
    eval_dataset = load_validation_data(
        config.DATA_DIR,
        #"./data/valid.json",
        tokenizer,
        config.MAX_SEQ_LENGTH
    )
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("\nåˆå§‹åŒ–æ¨¡å‹...")
    torch.cuda.set_device(0)
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=config.USE_NESTED_QUANT,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_NAME,
        quantization_config=bnb_config,
        device_map={'': 0},
        trust_remote_code=True,
        torch_dtype=compute_dtype,
        use_cache=False,
    )
    
    # å‡†å¤‡è®­ç»ƒ
    model = prepare_model_for_kbit_training(
        model,
        use_gradient_checkpointing=config.USE_GRADIENT_CHECKPOINTING
    )
    
    if config.USE_GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
    
    # æ·»åŠ LoRA
    peft_config = LoraConfig(
        r=config.LORA_R,
        lora_alpha=config.LORA_ALPHA,
        lora_dropout=config.LORA_DROPOUT,
        target_modules=config.LORA_TARGET_MODULES,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, peft_config)
    model.config.pad_token_id = tokenizer.eos_token_id
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"  åŸºæ¨¡å‹: {config.MODEL_NAME}")
    print(f"  LoRA Rank: {config.LORA_R}")
    print(f"  LoRA Alpha: {config.LORA_ALPHA}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    
    # ä¿å­˜é…ç½®
    config_dict = {
        "method": "centralized",
        "model": config.MODEL_NAME,
        "lora_config": {
            "r": config.LORA_R,
            "alpha": config.LORA_ALPHA,
            "dropout": config.LORA_DROPOUT,
            "target_modules": config.LORA_TARGET_MODULES
        },
        "training_config": {
            "num_epochs": config.NUM_EPOCHS,
            "batch_size": config.BATCH_SIZE,
            "gradient_accumulation_steps": config.GRADIENT_ACCUMULATION_STEPS,
            "effective_batch_size": config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS,
            "learning_rate": config.LEARNING_RATE,
            "max_seq_length": config.MAX_SEQ_LENGTH,
        },
        "data_config": {
            "train_file": "new_train_data_message.json",
            "validation_file": "valid.json",
            "total_train_samples": len(train_dataset),
            "total_eval_samples": len(eval_dataset) if eval_dataset is not None else 0
        },
        "note": "è¶…å‚æ•°ä¸è”é‚¦å­¦ä¹ å®Œå…¨ä¸€è‡´ï¼Œç”¨äºå…¬å¹³å¯¹æ¯”"
    }
    
    with open(f"{config.OUTPUT_DIR}/config.json", "w", encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    # è®­ç»ƒ
    model, training_history = train_centralized(
        model,
        tokenizer,
        train_dataset,
        eval_dataset,  # æ·»åŠ éªŒè¯é›†
        config,
        precision_info
    )
    
    # ä¿å­˜æ¨¡å‹
    print(f"\nä¿å­˜æ¨¡å‹...")
    model.save_pretrained(f"{config.OUTPUT_DIR}/final_model")
    tokenizer.save_pretrained(f"{config.OUTPUT_DIR}/final_model")
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {config.OUTPUT_DIR}/final_model")
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open(f"{config.OUTPUT_DIR}/training_history.json", "w", encoding='utf-8') as f:
        json.dump(training_history, f, indent=2, ensure_ascii=False)
    
    # ç»“æœæ€»ç»“
    print("\n" + "="*70)
    print("è®­ç»ƒç»“æœæ€»ç»“")
    print("="*70)
    print(f"è®­ç»ƒæ–¹æ³•: é›†ä¸­å¼è®­ç»ƒ (Centralized)")
    print(f"æ¨¡å‹: {config.MODEL_NAME}")
    print(f"æ•°æ®: å®Œæ•´è®­ç»ƒé›† (new_train_data.json)")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset is not None:
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")
    print(f"è®­ç»ƒè½®æ¬¡: {config.NUM_EPOCHS}")
    print(f"æœ€ç»ˆè®­ç»ƒLoss: {training_history['final_train_loss']:.4f}")
    if eval_dataset is not None and training_history['final_eval_metrics']:
        print(f"æœ€ç»ˆéªŒè¯Loss: {training_history['final_eval_metrics']['eval_loss']:.4f}")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {training_history['total_training_time']:.1f}ç§’")
    print(f"\nç»“æœä¿å­˜åœ¨: {config.OUTPUT_DIR}/")
    print("="*70)
    
    print("\nğŸ’¡ æç¤º:")
    print("  - æ­¤é›†ä¸­å¼è®­ç»ƒä½¿ç”¨ä¸è”é‚¦å­¦ä¹ å®Œå…¨ç›¸åŒçš„è¶…å‚æ•°")
    print("  - ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒé›†ï¼ˆè”é‚¦å­¦ä¹ ä¸­è¢«åˆ†å‰²ä¸ºå¤šä¸ªå®¢æˆ·ç«¯ï¼‰")
    print("  - å¯ä»¥ä¸è”é‚¦å­¦ä¹ ç»“æœè¿›è¡Œå…¬å¹³å¯¹æ¯”")
    if eval_dataset is not None:
        print("  - å·²ä½¿ç”¨éªŒè¯é›†ç›‘æ§è®­ç»ƒè´¨é‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")
        print("  - è‡ªåŠ¨ä¿å­˜éªŒè¯Lossæœ€ä½çš„æ¨¡å‹")
    print("  - å¯¹æ¯”æŒ‡æ ‡: è®­ç»ƒLossã€éªŒè¯Lossã€è®­ç»ƒæ—¶é—´ã€æ”¶æ•›é€Ÿåº¦ç­‰")


if __name__ == "__main__":
    main()